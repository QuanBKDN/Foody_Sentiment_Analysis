{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrowserOption(Enum):\n",
    "    \"\"\"Option for webbrowser\n",
    "    \"\"\"\n",
    "    EDGE = 1\n",
    "    CHROME = 2\n",
    "    FIREFOX = 3\n",
    "    SAFARI = 4\n",
    "\n",
    "\n",
    "\n",
    "class FileHandler():\n",
    "    @staticmethod\n",
    "    def is_file_empty(file_name: str) -> bool:\n",
    "        \"\"\"Return True if file is empty\n",
    "\n",
    "        Args: \n",
    "            - file_name: file's name that is needed to be check\n",
    "        \"\"\"\n",
    "        return os.stat(file_name).st_size == 0\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_csv(reviews: list[WebElement], points: list[WebElement], file_name: str) -> None:\n",
    "        header = ['Review', 'Point']\n",
    "        file = open(file_name, 'a', encoding='UTF8', newline='')\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if FileHandler.is_file_empty(file_name):\n",
    "            writer.writerow(header)\n",
    "        for review, point in zip(reviews, points):\n",
    "            row = [review.text, point.text]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "class FoodyCrawler():\n",
    "    @staticmethod\n",
    "    def get_driver(browser_option: BrowserOption = BrowserOption.CHROME):\n",
    "        \"\"\"Return driver depended on BrowserOption Enum\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "        \"\"\"\n",
    "        if browser_option == BrowserOption.EDGE:\n",
    "            options = webdriver.EdgeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.ChromiumEdge(options=options)\n",
    "        elif browser_option == BrowserOption.FIREFOX:\n",
    "            return webdriver.Firefox()\n",
    "        elif browser_option == BrowserOption.SAFARI:\n",
    "            return webdriver.Safari()       \n",
    "        else:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "            options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "            return webdriver.Chrome(options=options)  \n",
    "\n",
    "\n",
    "    def __init__(self, browser_option: BrowserOption, url_file: str, file_name_to_save: str ) -> None:\n",
    "        \"\"\"Create a new instance of FoodyCrawler\n",
    "        \n",
    "        Args:\n",
    "            - browser_option: the option of browser's driver\n",
    "            - url_file: a file that stores a list of url linked to restaurants\n",
    "            - file_name_to_save: file's name to save data crawled from links in url_file\n",
    "        \"\"\"\n",
    "        self.driver = FoodyCrawler.get_driver(browser_option)\n",
    "        self.url_file = url_file\n",
    "        self.file_name_to_save = file_name_to_save\n",
    "\n",
    "\n",
    "    def crawl(self, url: str) -> tuple[list[WebElement], list[WebElement]]:\n",
    "        \"\"\"Crawl data from single url\n",
    "        \n",
    "        Args:\n",
    "            - url: a url linked to a restaurant\n",
    "        \"\"\"\n",
    "        url = url+'/binh-luan'\n",
    "        self.driver.get(url)\n",
    "        self.driver.maximize_window()\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                view_review_button = self.driver.find_element(By.PARTIAL_LINK_TEXT, \"Xem thêm bình luận\")\n",
    "                self.driver.execute_script(\"arguments[0].click()\", view_review_button)\n",
    "                time.sleep(1)\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView();\", view_review_button)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "        review_selector = \"div.review-des > div.rd-des > span\"\n",
    "        point_selector = \"div.review-user > div > div.review-points > span\"\n",
    "\n",
    "        reviews = self.driver.find_elements(By.CSS_SELECTOR, review_selector)\n",
    "        points = self.driver.find_elements(By.CSS_SELECTOR, point_selector)\n",
    "        return reviews, points\n",
    "    \n",
    "\n",
    "    def start_crawling(self) -> None:\n",
    "        \"\"\"Start crawling data from url_link\"\"\"\n",
    "        with open(self.url_file, 'r') as file:\n",
    "            url_list = file.readlines()\n",
    "            for url in url_list:\n",
    "                print(url)\n",
    "                reviews, points = self.crawl(url)\n",
    "                FileHandler.write_to_csv(reviews, points, self.file_name_to_save)\n",
    "                self.driver.execute_script(\"window.open('');\")\n",
    "                self.driver.close()\n",
    "                self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foody = FoodyCrawler(browser_option=BrowserOption.EDGE, url_file='restaurants.txt', file_name_to_save='data_1.csv')\n",
    "foody.start_crawling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
